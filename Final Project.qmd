---
title: "Predicting Sale Price of Houses in Ames, IA with Linear Models"
author: "Kevin Sanford, Maile Kamada, and Harshaan Sall"
date: "12-13-2024"
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
    code-line-numbers: true
  pdf:
    code-line-numbers: true
editor: visual
---

## Introduction

We intend to use linear regression techniques to approach the exercise in the Kaggle competition found at [https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data). The exercise provides two large data sets that describes houses in Ames, Iowa with 79 parameters. The goal of the exercise is to create an algorithm that predicts the selling price of homes based on the parameters. The data provided consists of two data sets: test.csv and train.csv. The training data set includes housing prices to facilitate the creation of a linear regression model that predicts sale prices of homes, which can then be tested on the test data set. Before using the test data set, we will attempt to maximize the accuracy of the model by splitting the training data in half, using half of the data for training, then comparing the other half with the model's prediction. After tuning the model, we will retrain it on the full training data set and predict housing prices for the test data set.We begin by importing the data sets and previewing them.

In addition to the linear regression model, we will be training a Random Forest model. Random Forest is an ensemble learning technique that builds multiple decision trees and combines their outputs to capture complex, non-linear relationships in the data. To evaluate its performance, we will split the training data in half, using one half for training and the other for validation, measuring performance with metrics such as RMSE and R-squared.

## Data Analysis

### Import Data and Necessary Packages

```{r}
library(tidyverse)
```

```{r}
# check out data structure
housing_data.train <- read.csv("train.csv")
housing_data.test <- read.csv("test.csv")
head(housing_data.train)
```

### View Summary of Data

Using `summary()` with our data, we can see specific characteristics of the fields with numeric values, such as LotArea and GrLivArea. We can see statistics which will help us tune our model by potentially removing outliers if needed.

```{r}
summary(housing_data.train)
```

### Continuous Fields - Visualize Data with R Plots

To further view the continuous fields in our data, we plotted the values of those fields compared to the sale price. By plotting the data, we can better visualize any outliers. For example, in the plot for Lot Area, we can see that most of the values are less than 50000, so it is safe to assume that any houses above that area will probably affect our model negatively.

```{r}
# some numerical plots
plot(housing_data.train$YearBuilt,
     housing_data.train$SalePrice,
     xlab = "Year Built",
     ylab = "Sale Price")
plot(housing_data.train$LotArea,
     housing_data.train$SalePrice,
     xlab = "Lot Area",
     ylab = "Sale Price")
plot(housing_data.train$LotFrontage,
     housing_data.train$SalePrice,
     xlab = "Lot Frontage",
     ylab = "Sale Price")
plot(housing_data.train$GrLivArea,
     housing_data.train$SalePrice,
     xlab = "Living Area Square Feet",
     ylab = "Sale Price")
```

### Continuous Fields - Visualize Data with ggplot2

We can view the data with the median sale price, as well as the trend lines with standard error.

With `ggplot2()` we can take a closer look at the continuous fields for a more enhanced visualization. `geom_point()` creates a scatter plot of our data. `geom_hline()` creates a reference line using the median value of the sale price. `geom_smooth()` creates a shading around the reference depending on the method parameter provided. With these extra features, we have a better idea of how the data is distributed.

```{r}
# some numerical plots using ggplot2
ggplot(housing_data.train,
        aes(x = YearBuilt,
            y = SalePrice/1000)
) + geom_point() +
  geom_hline(yintercept = median(housing_data.train$SalePrice/1000)) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(y = "Sale Price ($1000's)",
       x = "Year Built",
       title = "Year Built vs. Sale Price")

ggplot(housing_data.train,
        aes(x = LotArea,
            y = SalePrice/1000)
) + geom_point() +
  geom_hline(yintercept = median(housing_data.train$SalePrice/1000)) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(y = "Sale Price ($1000's)",
       x = "Lot Area",
       title = "Lot Area vs. Sale Price")

housing_data.train %>%
  filter(LotArea < 50000) %>%
  ggplot(aes(x = LotArea,
             y = SalePrice/1000)) +
  geom_point() +
  geom_hline(yintercept = median(housing_data.train$SalePrice/1000)) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(y = "Sale Price ($1000's)",
       x = "Lot Area (Less than 50k)",
       title = "Lot Area Less Than 50k vs. Sale Price")

ggplot(housing_data.train,
        aes(x = LotFrontage,
            y = SalePrice/1000)
) + geom_point() +
  geom_hline(yintercept = median(housing_data.train$SalePrice/1000)) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(y = "Sale Price ($1000's)",
       x = "Lot Frontage",
       title = "Lot Frontage vs. Sale Price")

ggplot(housing_data.train,
        aes(x = GrLivArea,
            y = SalePrice/1000)
) + geom_point() +
  geom_hline(yintercept = median(housing_data.train$SalePrice/1000)) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(y = "Sale Price ($1000's)",
       x = "Living Area Square Feet",
       title = "Living Area Square Feet vs. Sale Price")
```

These plots show some positive correlation, suggesting that several factors are important for the cost of a home in Ames. Now we look at some categorical factors that may also affect the cost.

### Categorical Fields - Visualize Data with R Plots

We can also plot the data of categorical fields using `factor().` This function can be applied to categorical data to encode the character value and apply an integer to it for the count in the data. By plotting the factor, R creates a box plot to display the same values as the `summary()` did for the numeric fields.

```{r}
#explore categorical fields
plot(factor(housing_data.train$Neighborhood),
     housing_data.train$SalePrice,
     xlab = "Neighborhood",
     ylab = "Sale Price")
plot(factor(housing_data.train$Street),
     housing_data.train$SalePrice,
     xlab = "Type of Street",
     ylab = "Sale Price")
plot(factor(housing_data.train$Heating),
     housing_data.train$SalePrice,
     xlab = "Heating Type",
     ylab = "Sale Price")
plot(factor(housing_data.train$BldgType),
     housing_data.train$SalePrice,
     xlab = "Building Type",
     ylab = "Sale Price")
```

Now we use ggplot2 to to combine some of the numerical and categorical plots

### Categorical Fields - Visualize Data with ggplot2

We apply the same `ggplot2()` plots to the categorical fields as we did with the continuous fields.

```{r}
# combine plots of different variables with ggplot
# plot year built vs sale price with different neighbrhoods colored
ggplot(housing_data.train,
        aes(x = YearBuilt,
            y = SalePrice/1000,
            color = Neighborhood)
) + geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(y = "Sale Price ($1000's)",
       x = "Year Built",
       title = "Year Built vs. Sale Price")

# plot year built vs sale price for each neighborhood
housing_data.train %>%
  ggplot(aes(x = YearBuilt,
             y = SalePrice/1000)) +
  geom_point() +
  facet_wrap(~Neighborhood) +
  geom_hline(yintercept = median(housing_data.train$SalePrice/1000)) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(y = "Sale Price ($1000's)",
       x = "Year Built",
       title = "Year Built vs. Sale Price in Each Neighborhood")

# plot only 9 or less of the plots above for readability
housing_data.train %>%
  filter(Neighborhood != "IDOTRR") %>%
  filter(Neighborhood != "MeadowV") %>%
  filter(Neighborhood != "Mitchel") %>%
  filter(Neighborhood != "NAmes") %>%
  filter(Neighborhood != "NoRidge") %>%
  filter(Neighborhood != "NPkVill") %>%
  filter(Neighborhood != "NridgHt") %>%
  filter(Neighborhood != "NWAmes") %>%
  filter(Neighborhood != "OldTown") %>%
  filter(Neighborhood != "Sawyer") %>%
  filter(Neighborhood != "SawyerW") %>%
  filter(Neighborhood != "Somerst") %>%
  filter(Neighborhood != "StoneBr") %>%
  filter(Neighborhood != "SWISU") %>%
  filter(Neighborhood != "Timber") %>%
  filter(Neighborhood != "Veenker") %>%
  ggplot(aes(x = YearBuilt,
             y = SalePrice/1000)) +
  geom_point() +
  facet_wrap(~Neighborhood) +
  geom_hline(yintercept = median(housing_data.train$SalePrice/1000)) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(y = "Sale Price ($1000's)",
       x = "Year Built",
       title = "Year Built vs. Sale Price in Each Neighborhood")
```

```{r}
housing_data.train %>%
  filter(Neighborhood != "Blmngtn") %>%
  filter(Neighborhood != "Blueste") %>%
  filter(Neighborhood != "BrDale") %>%
  filter(Neighborhood != "BrkSide") %>%
  filter(Neighborhood != "ClearCr") %>%
  filter(Neighborhood != "CollgCr") %>%
  filter(Neighborhood != "Crawfor") %>%
  filter(Neighborhood != "Edwards") %>%
  filter(Neighborhood != "Gilbert") %>%
  filter(Neighborhood != "Sawyer") %>%
  filter(Neighborhood != "SawyerW") %>%
  filter(Neighborhood != "Somerst") %>%
  filter(Neighborhood != "StoneBr") %>%
  filter(Neighborhood != "SWISU") %>%
  filter(Neighborhood != "Timber") %>%
  filter(Neighborhood != "Veenker") %>%
  ggplot(aes(x = YearBuilt,
             y = SalePrice/1000)) +
  geom_point() +
  facet_wrap(~Neighborhood) +
  geom_hline(yintercept = median(housing_data.train$SalePrice/1000)) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(y = "Sale Price ($1000's)",
       x = "Year Built",
       title = "Year Built vs. Sale Price in Each Neighborhood")
```

```{r}
housing_data.train %>%
  filter(Neighborhood != "Blmngtn") %>%
  filter(Neighborhood != "Blueste") %>%
  filter(Neighborhood != "BrDale") %>%
  filter(Neighborhood != "BrkSide") %>%
  filter(Neighborhood != "ClearCr") %>%
  filter(Neighborhood != "CollgCr") %>%
  filter(Neighborhood != "Crawfor") %>%
  filter(Neighborhood != "Edwards") %>%
  filter(Neighborhood != "Gilbert") %>%
  filter(Neighborhood != "IDOTRR") %>%
  filter(Neighborhood != "MeadowV") %>%
  filter(Neighborhood != "Mitchel") %>%
  filter(Neighborhood != "NAmes") %>%
  filter(Neighborhood != "NoRidge") %>%
  filter(Neighborhood != "NPkVill") %>%
  filter(Neighborhood != "NridgHt") %>%
  filter(Neighborhood != "NWAmes") %>%
  filter(Neighborhood != "OldTown") %>%
  ggplot(aes(x = YearBuilt,
             y = SalePrice/1000)) +
  geom_point() +
  facet_wrap(~Neighborhood) +
  geom_hline(yintercept = median(housing_data.train$SalePrice/1000)) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(y = "Sale Price ($1000's)",
       x = "Year Built",
       title = "Year Built vs. Sale Price in Each Neighborhood")
```

```{r}
# plot year built vs sale price for each building type
housing_data.train %>%
  ggplot(aes(YearBuilt, SalePrice/1000, color = BldgType)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(y = "Sale Price ($1000's)",
       x = "Year Built",
       title = "Year Built vs. Sale Price",
       color = "Building Type")
```

```{r}
# plot building type vs sale price, separated by street type
ggplot(housing_data.train,
        aes(x = Street,
            y = SalePrice/1000,
            color = BldgType)
) + geom_boxplot() +
  geom_hline(yintercept = median(housing_data.train$SalePrice/1000)) +
  labs(y = "Sale Price ($1000's)",
       x = "Street Type",
       title = "Building Type vs. Sale Price",
       color = "Building Type")
```

Some of the data from the plots are not insightful. We can see that the home prices in certain neighborhoods have changed over time and some have not. Some neighborhoods have very few data points, illustrating the fact that no one variable can be relied upon too heavily when predicting housing prices.

## Prepare Data for the Model

Some of the data is not useful, such as the "Utilities" column, which has all but one identical entries.

```{r}
# find entries that are different
sum(housing_data.train$Utilities != "AllPub")

# delete the Utilities column in both train and test data sets
housing_data.train <- housing_data.train[, !(names(housing_data.train) %in% "Utilities")]
housing_data.test <- housing_data.test[, !(names(housing_data.test) %in% "Utilities")]
```

Now we look for missing values

```{r}
# find total number of NA values in Lot Frontage column
print("NA values in Lot Frontage:")
sum(is.na(housing_data.train$LotFrontage))

# find total number of NA values in Alley column
print("NA values in Alley:")
sum(is.na(housing_data.train$Alley))

# find total number of NA values in MasVnrArea column
print("NA values in MasVnrArea:")
sum(is.na(housing_data.train$MasVnrArea))

# find total number of NA values in Basement Quality column
print("NA values in Basement Quality:")
sum(is.na(housing_data.train$BsmtQual))

# find total number of NA values in Basement Condition column
print("NA values in Basement Condition:")
sum(is.na(housing_data.train$BsmtCond))

# find total number of NA values in Basement Exposure column
print("NA values in Basement Exposure:")
sum(is.na(housing_data.train$BsmtExposure))

# find total number of NA values in Basement Finish Type 1 column
print("NA values in Basement Finish Type 1:")
sum(is.na(housing_data.train$BsmtFinType1))

# find total number of NA values in Basement Finish Type 2 column
print("NA values in Basement Finish Type 2:")
sum(is.na(housing_data.train$BsmtFinType2))

# find total number of NA values in Electrical column
print("NA values in Electrical:")
sum(is.na(housing_data.train$Electrical))

# find total number of NA values in Fireplace Quality column
print("NA values in Fireplace Quality:")
sum(is.na(housing_data.train$FireplaceQu))

# find total number of NA values in Garage Type column
print("NA values in Garage Type:")
sum(is.na(housing_data.train$GarageType))

# find total number of NA values in Garage Year Built column
print("NA values in Garage Year Built:")
sum(is.na(housing_data.train$GarageYrBlt))

# find total number of NA values in Garage Finish column
print("NA values in Garage Finish:")
sum(is.na(housing_data.train$GarageFinish))

# find total number of NA values in Garage Quality column
print("NA values in Garage Quality:")
sum(is.na(housing_data.train$GarageQual))

# find total number of NA values in Garage Condition column
print("NA values in Garage Condition:")
sum(is.na(housing_data.train$GarageCond))

# find total number of NA values in Pool QC column
print("NA values in Pool QC:")
sum(is.na(housing_data.train$PoolQC))

# find total number of NA values in Fence column
print("NA values in Fence:")
sum(is.na(housing_data.train$Fence))

# find total number of NA values in Misc. Features column
print("NA values in Misc. Features:")
sum(is.na(housing_data.train$MiscFeature))
```

A column with too many missing values will not be useful to us. We remove columns with \> 100 missing values. For the others, replace NAs with "None."

```{r}
# remove columns with too many missing values
housing_data.train <- housing_data.train[, !(names(housing_data.train) %in% c("LotFrontage", "Alley", "FireplaceQu", "PoolQC", "Fence", "MiscFeature"))]
housing_data.test <- housing_data.test[, !(names(housing_data.test) %in% c("LotFrontage", "Alley", "FireplaceQu", "PoolQC", "Fence", "MiscFeature"))]

# replace NAs with the mode
housing_data.train$MasVnrArea[is.na(housing_data.train$MasVnrArea)] <- names(sort(-table(housing_data.train$MasVnrArea)))[1]

# replace NAs with the mode
housing_data.train$BsmtQual[is.na(housing_data.train$BsmtQual)] <- names(sort(-table(housing_data.train$BsmtQual)))[1]

# replace NAs with the mode
housing_data.train$BsmtCond[is.na(housing_data.train$BsmtCond)] <- names(sort(-table(housing_data.train$BsmtCond)))[1]

# replace NAs with the mode
housing_data.train$BsmtExposure[is.na(housing_data.train$BsmtExposure)] <- names(sort(-table(housing_data.train$BsmtExposure)))[1]

# replace NAs with the mode
housing_data.train$BsmtFinType1[is.na(housing_data.train$BsmtFinType1)] <- names(sort(-table(housing_data.train$BsmtFinType1)))[1]

# replace NAs with the mode
housing_data.train$BsmtFinType2[is.na(housing_data.train$BsmtFinType2)] <- names(sort(-table(housing_data.train$BsmtFinType2)))[1]

# replace NAs with the mode
housing_data.train$Electrical[is.na(housing_data.train$Electrical)] <- names(sort(-table(housing_data.train$Electrical)))[1]

# replace NAs with the mode
housing_data.train$GarageType[is.na(housing_data.train$GarageType)] <- names(sort(-table(housing_data.train$GarageType)))[1]

# replace NAs with the mode
housing_data.train$GarageYrBlt[is.na(housing_data.train$GarageYrBlt)] <- names(sort(-table(housing_data.train$GarageYrBlt)))[1]

# replace NAs with the mode
housing_data.train$GarageFinish[is.na(housing_data.train$GarageFinish)] <- names(sort(-table(housing_data.train$GarageFinish)))[1]

# replace NAs with the mode
housing_data.train$GarageQual[is.na(housing_data.train$GarageQual)] <- names(sort(-table(housing_data.train$GarageQual)))[1]

# replace NAs with the mode
housing_data.train$GarageCond[is.na(housing_data.train$GarageCond)] <- names(sort(-table(housing_data.train$GarageCond)))[1]
```

Now, we split the training data and use half of it to train a linear regression model. When the model is trained, we can test its accuracy with the other half. Since the test data did not provide the sale price, we instead will use the second half of the training data as our "test".

```{r}
# split the train data set in half
housing_data.train.1 <- housing_data.train[1:730,]
housing_data.train.2 <- housing_data.train[731:1460,]
head(housing_data.train.1)
head(housing_data.train.2)
```

### Ensure Normal Distribution

Now we check that the sale price has normal distribution, ensuring that we meet the assumptions to use the linear model. We do this by viewing Q-Q plot, with `qqnorm()` and `qqline()`. If the data follows a linear pattern with the qq line, then it has a normal distribution. If not, then we need to transform the data to ensure normal distribution.

```{r}
#view Q-Q plot to check if Sale Price has a normal distribution
qqnorm(housing_data.train.1$SalePrice)
qqline(housing_data.train.1$SalePrice)
```

Since the data does not follow normal distribution, we can apply the log function transform the data. This follows the same technique as the paper we are references for this project.

```{r}
#apply log to the sale price like in the source paper to ensure normal dist
qqnorm(log(housing_data.train.1$SalePrice))
qqline(log(housing_data.train.1$SalePrice))
```

### Tests for Significant Variables

Now that we know our data has a normal distribution, we can apply hypothesis testing such as `t.test()` and correlation `cor().`

#### T Tests

As shown in lecture, we can view which variables are significant using a t test `t.test()`. In the output, if the p-value is low, then we can assume that the variable has statistical significance. Below, we use the Neighborhood field as an example. We can see which neighborhood will be more significant in our model.

```{r}
# check to make sure certain variables are significant
t.test (log(housing_data.train.1$SalePrice[housing_data.train.1$Neighborhood == 'NAmes']))
t.test (log(housing_data.train.1$SalePrice[housing_data.train.1$Neighborhood == 'CollgCr']))
t.test (log(housing_data.train.1$SalePrice[housing_data.train.1$Neighborhood == 'OldTown']))
t.test (log(housing_data.train.1$SalePrice[housing_data.train.1$Neighborhood == 'Edwards']))
t.test (log(housing_data.train.1$SalePrice[housing_data.train.1$Neighborhood == 'NoRidge']))
t.test (log(housing_data.train.1$SalePrice[housing_data.train.1$Neighborhood == 'NridgHt']))
t.test (log(housing_data.train.1$SalePrice[housing_data.train.1$Neighborhood == 'Somerst']))
t.test (log(housing_data.train.1$SalePrice[housing_data.train.1$Neighborhood == 'StoneBr']))
```

A small p-value in these t-tests show that the neighborhood affects the sale price, so we should include neighborhood data in our model. We also apply the `t.test()` on the Lot Area field while removing the outliers of any houses over 50,000 sq ft.

```{r}
t.test (log(housing_data.train.1$SalePrice))
t.test (log(housing_data.train.1$SalePrice[housing_data.train.1$LotArea < 50000]))
t.test (log(housing_data.train.1$SalePrice[housing_data.train.1$GrLivArea]))
```

#### Correlation

Another way to measure significance is to use correlation `cor()`. We apply the correlation function on various fields in our data. The higher the correlation, the stronger the relationship is between the variable and the sale price.

```{r}
print("Correlation between sale price and overall quality:")
cor(log(housing_data.train.1$SalePrice),
    housing_data.train.1$OverallQual)

print("Correlation between sale price and lot area:")
cor(log(housing_data.train.1$SalePrice),
    housing_data.train.1$LotArea)

print("Correlation between sale price and lot area < 50k:")
cor(log(housing_data.train.1$SalePrice[housing_data.train.1$LotArea < 50000]),
    housing_data.train.1$LotArea[housing_data.train.1$LotArea < 50000])

print("Correlation between sale price and living room area:")
cor(log(housing_data.train.1$SalePrice),
    housing_data.train.1$GrLivArea)

print("Correlation between sale price and year built:")
cor(log(housing_data.train.1$SalePrice),
    housing_data.train.1$YearBuilt)

print("Correlation between sale price and year remodeled:")
cor(log(housing_data.train.1$SalePrice),
    housing_data.train.1$YearRemodAdd)

print("Correlation between sale price and basement size:")
cor(log(housing_data.train.1$SalePrice),
    housing_data.train.1$TotalBsmtSF)
```

Overall quality score has the highest correlation with sale price. Living room area, garage area, and floor sizes seem to be some of the other highest correlated factors with sale price. Now we check if these are normal distributions.

```{r}
# view Q-Q plot of overall quality
qqnorm(housing_data.train.1$OverallQual)
qqline(housing_data.train.1$OverallQual)

# view Q-Q plot of living room size
qqnorm(housing_data.train.1$GrLivArea)
qqline(housing_data.train.1$GrLivArea)

# view Q-Q plot of garage size, remove zeros
qqnorm(housing_data.train.1$GarageArea[housing_data.train.1$GarageArea > 0])
qqline(housing_data.train.1$GarageArea[housing_data.train.1$GarageArea > 0])

# view Q-Q plot of basement size, remove zeros
qqnorm(housing_data.train.1$TotalBsmtSF[housing_data.train.1$TotalBsmtSF > 0])
qqline(housing_data.train.1$TotalBsmtSF[housing_data.train.1$TotalBsmtSF > 0])

# view Q-Q plot of 1st floor size
qqnorm(housing_data.train.1$X1stFlrSF)
qqline(housing_data.train.1$X1stFlrSF)

# view Q-Q plot of 2nd floor size, remove zeros
qqnorm(housing_data.train.1$X2ndFlrSF[housing_data.train.1$X2ndFlrSF > 0])
qqline(housing_data.train.1$X2ndFlrSF[housing_data.train.1$X2ndFlrSF > 0])
```

Now we apply the log function to find normal distribution

```{r}
# apply log to overall quality
qqnorm(log(housing_data.train.1$OverallQual))
qqline(log(housing_data.train.1$OverallQual))

# apply log to living room size
qqnorm(log(housing_data.train.1$GrLivArea))
qqline(log(housing_data.train.1$GrLivArea))

# apply log to garage size without zeros
qqnorm(log(housing_data.train.1$GarageArea[housing_data.train.1$GarageArea > 0]))
qqline(log(housing_data.train.1$GarageArea[housing_data.train.1$GarageArea > 0]))

# apply log to basement size without zeros
qqnorm(log(housing_data.train.1$TotalBsmtSF[housing_data.train.1$TotalBsmtSF > 0]))
qqline(log(housing_data.train.1$TotalBsmtSF[housing_data.train.1$TotalBsmtSF > 0]))

# apply log to 1st floor size
qqnorm(log(housing_data.train.1$X1stFlrSF))
qqline(log(housing_data.train.1$X1stFlrSF))

# apply log to 2nd floor size without zeros
qqnorm(log(housing_data.train.1$X2ndFlrSF[housing_data.train.1$X2ndFlrSF > 0]))
qqline(log(housing_data.train.1$X2ndFlrSF[housing_data.train.1$X2ndFlrSF > 0]))
```

It seems like the log function of living room area, garage area, and 1st floor area are good fits to a normal distribution. Overall quality fits a normal distribution best without a log function.

## Apply Linear Regression Model

First, we apply the `lm()` function to the living room area, which was shown to be a significant factor in the house sale price. This function will fit the linear model provided the data given.

```{r}
#apply lm to living room area
housing.lm <- lm(log(housing_data.train.1$SalePrice) ~  housing_data.train.1$GrLivArea ,data=housing_data.train.1)

housing.lm
plot(housing.lm)
```

### Remove Outliers

In lecture, we learned to check the Residuals vs Fitted chart to see if the model needs adjusting. The red trend line should move along the dotted line in the chart. Since this is not the case, we can try removing outliers in the living room area and the sale price to enhance the model. We can also include the other significant fields to improve our model.

```{r}
#remove outliers in sale price
housing_data.train.noout <- housing_data.train.1[housing_data.train.1$SalePrice < 350000,]

#remove outliers in living room area
housing_data.train.noout <- housing_data.train.noout[housing_data.train.noout$GrLivArea < 3000,]

# remove outliers in garage area
housing_data.train.noout <- housing_data.train.noout[housing_data.train.noout$GarageArea > 0,]

housing.lm.2 <- lm(log(housing_data.train.noout$SalePrice)
                   ~ housing_data.train.noout$OverallQual
                   + housing_data.train.noout$Neighborhood
                   + housing_data.train.noout$YearBuilt
                   + housing_data.train.noout$YearRemodAdd
                   + housing_data.train.noout$TotalBsmtSF
                   + log(housing_data.train.noout$X1stFlrSF)
                   + housing_data.train.noout$X2ndFlrSF
                   + log(housing_data.train.noout$GrLivArea)
                   + log(housing_data.train.noout$GarageArea),
                   data = housing_data.train.noout)
housing.lm.2
plot(housing.lm.2)
```

We can see that the red line in the Residuals vs Fitted chart follow a straight line, which means that our adjustments worked!

Before we apply any predictions, let's view the `summary()` of this model:

```{r}
#view details of lm and predict sale price using the lm
summary.lm(housing.lm.2)
```

### Predict

Now that we created a model, we can use the other half of the training data to predict the sale price with `predict.lm()`.

```{r}
# match dimensions
total_row <- nrow(housing_data.train.noout)
housing_predict <- predict.lm(housing.lm.2,housing_data.train.2[1:total_row,])

```

## Checking Our Prediction

### Correlation

Using correlation, we can see how accurate our model was with the actual sale price in the training data.

```{r}
#see what the correlation is between the prediction and actuals
actual_sale_price <- (housing_data.train.2$SalePrice[1:total_row] %/% 1000) * 1000
predict_sale_price <- (exp(housing_predict) %/% 1000) * 1000

cor(actual_sale_price,predict_sale_price)

```

We can also visualize the plot between the actual sale price and the predictions from our model.

```{r}
# visualize correlation
plot(actual_sale_price, predict_sale_price,
     xlab = "Actual Sale Price ($1000s)",
     ylab = "Predicted Sale Price ($1000s)")
```

Random Forest Model Load in the dataset for preprocessing before training the random forest model.

```{r}
# split the train data set in half
housing_data.train.1 <- housing_data.train[1:730,]
housing_data.test.1 <- housing_data.train[731:1460,]
head(housing_data.train.1)
head(housing_data.test.1 )
```

```{r}
# Check for missing values in each column
colSums(is.na(housing_data.train.1))
colSums(is.na(housing_data.test.1))
```

```{r}
# Identify numeric columns
numeric_cols <- sapply(housing_data.train.1, is.numeric)

# Impute missing numeric values with the median
for (col in names(housing_data.train.1)[numeric_cols]) {
  if (all(is.na(housing_data.train.1[[col]]))) {
    housing_data.train.1[[col]][is.na(housing_data.train.1[[col]])] <- 0
    housing_data.test.1[[col]][is.na(housing_data.test.1[[col]])] <- 0
  } else {
    median_value <- median(housing_data.train.1[[col]], na.rm = TRUE)
    housing_data.train.1[[col]][is.na(housing_data.train.1[[col]])] <- median_value
    housing_data.test.1[[col]][is.na(housing_data.test.1[[col]])] <- median_value
  }
}
```

```{r}
# Identify categorical columns
categorical_cols <- sapply(housing_data.train.1, is.character)

# Impute missing categorical values with "Other"
for (col in names(housing_data.train.1)[categorical_cols]) {
  housing_data.train.1[[col]][is.na(housing_data.train.1[[col]])] <- "Other"
  housing_data.test.1[[col]][is.na(housing_data.test.1[[col]])] <- "Other"

  # Convert to factors and align levels
  housing_data.train.1[[col]] <- factor(housing_data.train.1[[col]])
  housing_data.test.1[[col]] <- factor(housing_data.test.1[[col]], levels = levels(housing_data.train.1[[col]]))
}
```

Now, we will train the random forest model.

```{r}
library(randomForest)

# Filter dataset to include only numerical predictors and the target variable
numerical_cols <- sapply(housing_data.train.1, is.numeric)  # Identify numerical columns
numerical_data <- housing_data.train.1[, numerical_cols]    # Subset only numerical columns
numerical_data$SalePrice <- housing_data.train.1$SalePrice  # Ensure the target variable is included

# Train the Random Forest model
set.seed(123)  # For reproducibility
rf_model <- randomForest(
  SalePrice ~ .,          # Use all numerical predictors
  data = numerical_data,
  ntree = 500,            # Number of trees
  mtry = floor(sqrt(ncol(numerical_data) - 1)),  # Number of features per split
  importance = TRUE       # Calculate feature importance
)

# View model summary
print(rf_model)
```

```{r}
# Predict on the test dataset
rf_predictions <- predict(rf_model, newdata = housing_data.test.1)
# Check for missing or invalid predictions
summary(rf_predictions)
```

```{r}
# Filter out NA predictions
valid_indices <- !is.na(rf_predictions)  # Get indices of non-NA predictions
filtered_predictions <- rf_predictions[valid_indices]
filtered_actuals <- housing_data.test.1$SalePrice[valid_indices]
```

```{r}
library(Metrics)

# Calculate RMSE
rmse_value <- rmse(filtered_actuals, filtered_predictions)

# Calculate the average sale price
average_sale_price <- mean(filtered_actuals)

# Print RMSE and the average sale price
cat("RMSE:", rmse_value, "\n")
cat("Average Sale Price:", average_sale_price, "\n")

# Calculate RMSE as a percentage of the average sale price
rmse_percentage <- (rmse_value / average_sale_price) * 100
cat("RMSE as Percentage of Average Sale Price:", rmse_percentage, "%\n")
```

```{r}
# Calculate R²
rsq_value <- 1 - sum((filtered_actuals - filtered_predictions)^2) / 
                 sum((filtered_actuals - mean(filtered_actuals))^2)
cat("R²:", rsq_value, "\n")
```

Given these metrics, we see the Random Forest model performed well with an R-squared value of .856. This indicated that the model explains around 85% of the variance in the prices of the homes and essentially captures a majority of the patterns. For the RMSE value of \$29,299.79, this indicated that on average the models predictions differ from the actual values by this amount. Given that this is only 16.37% of the average sales price, this indicates a good performing model.

```{r}
plot_data <- data.frame(
  Actual = filtered_actuals,
  Predicted = filtered_predictions
)

# Create the scatter plot
ggplot(plot_data, aes(x = Actual, y = Predicted)) +
  geom_point(color = "blue", alpha = 0.6) +  # Scatter points
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +  # 45-degree line
  labs(
    title = "Predicted vs. Actual Housing Prices",
    x = "Actual Sale Prices",
    y = "Predicted Sale Prices"
  ) +
  theme_minimal()
```
