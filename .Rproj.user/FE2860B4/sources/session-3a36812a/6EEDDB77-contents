---
title: "Homework 2"
author: "Kevin Sanford"
date: "11-7-2024"
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
    code-line-numbers: true
  pdf:
    code-line-numbers: true
error: true
cache: true
---

# Problem 1: Householder transformations

A Householder transformation (or Householder reflector) is a matrix

$$
\mathbf{H} = \mathbf{I} - 2 \mathbf{u} \mathbf{u}^\top,
$$

defined by a unit vector $\mathbf{u}$.

## Part (a)

(a) Show that $\mathbf{H} \mathbf{u} = -\mathbf{u}$, where $\mathbf{u}$ is the same vector defining $\mathbf{H}$.

**Solution**

$$
\mathbf{Hu} = \left(\mathbf{I} - 2 \mathbf{uu}^\top \right) \mathbf{u}
$$

$$
= \mathbf{Iu} - \left(2 \mathbf{uu}^\top\right)\mathbf{u}
$$

$$
= \mathbf{u} - 2 \mathbf{u} \left(\mathbf{u}^\top\mathbf{u}\right)
$$

Since $\mathbf{u}$ is a unit vector,

$$
\mathbf{u}^\top\mathbf{u} = 1
$$

Then we have:

$$
\mathbf{Hu} = \mathbf{u} - 2 \mathbf{u}
$$

$$
= -\mathbf{u}
$$

## Part (b)

(b) If $\mathbf{u} \perp \mathbf{v}$, show that $\mathbf{H} \mathbf{v} = \mathbf{v}$.

**Solution**

$$
\mathbf{Hv} = \left(\mathbf{I} - 2 \mathbf{uu}^\top\right) \mathbf{v}
$$

$$
= \mathbf{Iv} - \left(2 \mathbf{uu}^\top\right)\mathbf{v}
$$

$$
= \mathbf{v} - 2 \mathbf{u} \left(\mathbf{u}^\top\mathbf{v}\right)
$$

Since $\mathbf{u}$ and $\mathbf{v}$ are perpendicular,

$$
\mathbf{u}^\top\mathbf{v} = 0
$$

Then we have:

$$
\mathbf{Hv} = \mathbf{v}
$$

## Part (c)

(c) Show that $\mathbf{H}$ is symmetric and orthogonal.

**Solution**

$$
\mathbf{H}^\top = \left(\mathbf{I} - 2 \mathbf{u} \mathbf{u}^\top\right)^\top
$$

$$
= \mathbf{I}^\top - 2 \left(\mathbf{uu^\top}\right)^\top
$$

$$
= \mathbf{I} - 2 \mathbf{uu}^\top
$$

$$
= \mathbf{H}
$$

Since $\mathbf{H} = \mathbf{H}^\top$, $\mathbf{H}$ is a symmetric matrix. To prove that it is an orthogonal matrix, we find:

$$
\mathbf{H}^\top\mathbf{H} = \mathbf{HH}
$$

$$
= \left(\mathbf{I} - 2 \mathbf{u} \mathbf{u}^\top\right)^2
$$

$$
= \mathbf{I}^2 - \mathbf{I}\left(2 \mathbf{uu}^\top\right)
- \left(2 \mathbf{uu}^\top\right)\mathbf{I}
+ \left(4 \mathbf{uu}^\top\mathbf{uu}^\top\right)
$$

$$
= \mathbf{I} - 2 \mathbf{uu}^\top
- 2 \mathbf{uu}^\top + 4\mathbf{uu}^\top
$$

$$
= \mathbf{I}
$$

Since $\mathbf{uu}^\top$ makes a square matrix, $\mathbf{H}$ is also a square matrix, and since $\mathbf{H}^\top\mathbf{H} = \mathbf{I}$, then $\mathbf{H}$ is an orthogonal matrix.

## Part (d)

(d) Write a function called `householder` which *implements the action* of a Householder matrix, generated by `u`, on a vector `x`.

::: callout-tip
### Hints

1.  The function should take two arguments.
2.  **Do not** form $\mathbf{H}$ as a `matrix`.
:::

**Solution**

```{r}
householder <- function(u, x) {
  return (x - 2*u*sum(u*x))
}
```

## Part (e)

(e) Let `u = c(1, 0, 0)`. Test your function on `x = c(1, 0, 0)` and `x = c(0, 1, 0)`.

**Solution**

```{r}
u = c(1,0,0)
householder(u, c(1,0,0))
```

```{r}
householder(u, c(0,1,0))
```

# Problem 2: Making a matrix upper triangular

Householder transformations are a "minimal" solution to the problem of mapping a vector $\mathbf{x}$ into $\mathbf{y}$. This can be exploited to introduce subdiagonal zeros into a matrix to implement Gaussian elimination, transforming a matrix into an upper triangular form.

## Part (a)

(a) Given arbitrary **unit vectors** $\mathbf{x}$ and $\mathbf{y}$ in $\mathbb{R}^{n}$, show that the choice $\mathbf{u} = \frac{1}{2}(\mathbf{x} - \mathbf{y})$ allows us to map $\mathbf{x}$ into $\mathbf{y}$; that is, $\mathbf{H} \mathbf{x} = \mathbf{y}$.

::: callout-tip
## Hints

1.  Draw a picture to visualize the reflection to convince yourself that it should be the case.
2.  Observe that $\mathbf{x} = \frac{1}{2}(\mathbf{x} + \mathbf{y}) + \frac{1}{2}(\mathbf{x} - \mathbf{y})$. There is a similar expression for $\mathbf{y}$, which can be related to your picture.
:::

**Solution**

$$
\mathbf{Hx}
= \left(\mathbf{I} - 2 \mathbf{uu}^\top\right)\mathbf{x}
$$

$$
= \mathbf{x} - 2 \mathbf{uu}^\top\mathbf{x}
$$

$$
= \mathbf{x}
- 2 \left[
  \frac{1}{2} (\mathbf{x}-\mathbf{y})
\right]
\left[
  \frac{1}{2} (\mathbf{x}-\mathbf{y})
\right]^\top \mathbf{x}
$$

$$
= \mathbf{x}
- \frac{1}{2} (\mathbf{x} - \mathbf{y})
(\mathbf{x}^\top - \mathbf{y}^\top)
\mathbf{x}
$$

Since $\mathbf{x}$ and $\mathbf{y}$ are unit vectors, then $\mathbf{x}^\top\mathbf{x} = \mathbf{xx}^\top=\mathbf{yy}^\top=1$. Then we have:

$$
\mathbf{Hx} = \mathbf{x}
- \frac{1}{2} (\mathbf{x}-\mathbf{y})
(1-\mathbf{y}^\top\mathbf{x})
$$

$$
= \mathbf{x}
- \frac{1}{2} \left(
  \mathbf{x} - \mathbf{y} - \mathbf{xy}^\top\mathbf{x}
  + \mathbf{yy}^\top\mathbf{x}
\right)
$$

$$
= \mathbf{x}
- \frac{1}{2} \left(
  \mathbf{x} - \mathbf{y} - \mathbf{xx}^\top\mathbf{y}
  + \mathbf{yy}^\top\mathbf{x}
\right)
$$

$$
= \mathbf{x}
- \frac{1}{2} \left(
  \mathbf{x} - \mathbf{y} - \mathbf{y}
  + \mathbf{x}
\right)
$$

$$
= \mathbf{x} - \left(
  \mathbf{x} - \mathbf{y}
\right)
$$

$$
= \mathbf{y}
$$

## Part (b)

(b) With this in mind, consider a square matrix $\mathbf{X}$; for example:

```{r}
# Do not modify X: you will use it throghout the assignment.
X <- matrix(seq(1, 9), nrow = 3)
X
```

Use the previous observation to compute a vector `u` such that $$\mathbf{H} \mathbf{x}_{1} = \begin{bmatrix} \|\mathbf{x}_{1}\| \\ 0 \\ \vdots \\ 0 \end{bmatrix}.$$ Note that here we do not require $\mathbf{u}$ to be a unit vector.

**Solution**

Let

$$
\mathbf{y}_1 = \begin{bmatrix} \|\mathbf{x}_{1}\| \\ 0 \\ \vdots \\ 0 \end{bmatrix}
$$

To map $\mathbf{x}_1$ into $\mathbf{y}_1$ using $\mathbf{u} = \frac{1}{2}(\mathbf{x}_1-\mathbf{y}_1)$, we first find unit vectors for $\mathbf{x}_1$ and $\mathbf{y}_1$. Let

$$
\hat{\mathbf{x}}_1 = \frac{\mathbf{x}_1}{||\mathbf{x}_1||},
\quad \hat{\mathbf{y}}_1
= \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}
= \frac{\mathbf{y}_1}{||\mathbf{x}_1||}
$$

Then $\mathbf{H\hat{x}}_1 = \hat{\mathbf{y}}_1$, as above for $\mathbf{u} = \frac{1}{2}(\hat{\mathbf{x}}_1 - \hat{\mathbf{y}}_1)$. Since $||\mathbf{x_1}||$ is a scalar,

$$
\mathbf{H\hat{x}}_1 = \hat{\mathbf{y}}_1
= \mathbf{H{x}}_1 ||\mathbf{x}_1||
= \mathbf{y}_1 ||\mathbf{x}_1||
$$

So then $\mathbf{Hx}_1 = \mathbf{y}_1$ for $\mathbf{u} = \frac{1}{2}(\hat{\mathbf{x}}_1 - \hat{\mathbf{y}}_1)$.

## Part (c)

(c) Test your solution using `householder()` and column `X[, 1]`.

**Solution**

```{r}
# construct u
xnorm <- norm(as.matrix(X[,1]), type = "e")
yHat <- c(1, rep(0,length(X[,1])-1))
u <- 0.5 * (X[,1]/xnorm - yHat)
householder(u, X[,1])
```

Since this does not yield the expected result of $\mathbf{y}_1$, we try a normalized $\mathbf{u}$:

```{r}
# normalize u
u <- u / norm(as.matrix(u), type ="e")
householder(u, X[,1])
```

## Part (d)

(d) Compute $\mathbf{H} \mathbf{X}$ using `apply()`; that is, apply `householder()` to each column of `X`.

**Solution**

```{r}
apply(X,2,householder, u = u)
```

## Part (e)

(e) In general, we choose

$$
\mathbf{u}_{j} = \begin{bmatrix}
  \mathbf{0}_{j-1} \\
  \tilde{\mathbf{u}}_{j}
\end{bmatrix},
\qquad
\tilde{\mathbf{u}}_{j} \in \mathbb{R}^{n-j+1}
$$

to eliminate the subdiagonal of column $j$ of $\mathbf{X}$ as we move left to right. Specifically, the component $\tilde{\mathbf{u}}_{j}$ is a unit vector chosen to map sub-vector $\mathbf{z} = \mathbf{X}_{[j:n, j]} \in \mathbb{R}^{n-j+1}$ to $\begin{bmatrix} \|\mathbf{z}\| \\ 0 \\ \vdots \\ 0 \end{bmatrix}$ in $\mathbb{R}^{n-j+1}$. In other words, we ignore the previous rows and only operate on the remaining components of column $j$.

Write a function called `eliminate_col()` which

-   accepts a matrix `X` and index `j`, and
-   uses `householder()` to eliminate the subdiagonal on column `j`.

::: callout-tip
## Hint

Either vector `u` needs to be a unit vector, or the vectors in the difference (occuring in `u`) need to be unit vectors.
:::

**Solution**

```{r}
eliminate_col <- function(X, j) {
  # define z as a subvector of X[,j], from diagonal to end
  z <- X[,j][j:length(X[,j])]
  
  # construct u tilde
  znorm <- norm(as.matrix(z), type = "e")
  yHat <- c(1, rep(0,length(z)-1))
  uTilde <- 0.5 * (z/znorm - yHat)
  # normalize u tilde
  uTilde <- uTilde / norm(as.matrix(uTilde), type ="e")
  # construct u
  u <- c(rep(0, j-1), uTilde)
  
  # apply householder with this u to each column of X
  X <- apply(X,2,householder, u = u)
  
  return(X)
}
```

**Test 1**: Eliminate column 1 only. This should match the output above. *Do not modify* `X`.

```{r}
eliminate_col(X, 1)
#          [,1]     [,2]      [,3]
# [1,] 3.741657 8.552360 13.363062
# [2,] 0.000000 1.679118  3.358237
# [3,] 0.000000 1.018678  2.037355
```

**Test 2**: Eliminate column 2 only. *Do not modify* `X`.

```{r}
eliminate_col(X, 2)
#            [,1]          [,2]       [,3]
# [1,]  1.0000000  4.000000e+00  7.0000000
# [2,]  3.5850326  7.810250e+00 12.0354667
# [3,] -0.3841106 -1.776357e-15  0.3841106
```

# Problem 3: Implementing QR with Householder reflectors

The LAPACK package implements [QR decomposition with Householder reflectors](https://www.netlib.org/lapack/lug/node69.html), albeit using a sophisticated *blocking* approach that improves low-level data parallelism. We are now in position to implement this algorithm ourselves.

## Part (a)

(a) First let's implement the sequential elimination of columns.

-   Initialize a matrix `R` which is just a copy of `X`.
-   For each column `j` of `R`, eliminate column `j` and save the result to `R`.
-   Display the result.
-   **What do you see as the result?**
-   **Comment on what you think happened.**

**Your code**

```{r}
R <- X
for (j in 1:length(R[1,])) {
  R <- eliminate_col(R, j)
}
R
```

The matrix has all NaN values. For the third column, $\mathbf{u} = \mathbf{0}$ when $\mathbf{z}$ is length 1. When the function attempts to normalize $\mathbf{u}$, it divides by its norm, which results in a NaN.

## Part (b)

(b) Now let's try applying only the first 2 elimination steps.

-   Modify your code above to apply only 2 Householder transformations.
-   **What do you see as the result?**
-   **What do you notice about column 3?**

**Your code**

```{r}
R <- X
for (j in 1:2) {
  R <- eliminate_col(R, j)
}
R
```

I get the expected result. The last element in column 3 became 0.

## Part (c)

(c) Let's create a new version of our elimination function, called `eliminate_col2()`, which

-   accepts an argument `tol` with default value `sqrt(.Machine$double.eps)`,
-   checks whether `u` is a zero vector, and
-   returns a modified version of `X`, which may or may not reflect the result of applying a Householder transformation.

**Your code**

```{r}
eliminate_col2 <- function(X, j, tol = sqrt(.Machine$double.eps)) {
  # define z as a subvector of X[,j], from diagonal to end
  z <- X[,j][j:length(X[,j])]
  
  # calculate u tilde (unnormalized)
  znorm <- norm(as.matrix(z), type = "e")
  yHat <- c(1, rep(0,length(z)-1))
  uTilde <- 0.5 * (z/znorm - yHat)
  # check if u is 0, if it is return X unchanged
  if (sum(abs(uTilde) < tol) > 0 ) {
    return (X)
  }
  
  # normalize u tilde
  uTilde <- uTilde / norm(as.matrix(uTilde), type ="e")
  # construct u
  u <- c(rep(0, j-1), uTilde)
  
  # apply householder with this u to each column of X
  X <- apply(X,2,householder, u = u)
  
  return(X)
}
```

**Test it here** Apply `eliminate_col2()` to eliminate subdiagonals in every column.

```{r}
R <- X
for (j in 1:length(R[1,])) {
  R <- eliminate_col2(R, j)
}
R
```

## Part (d)

(d) The last part shows that we can put $X$ into upper triangular form using a sequence of Householder transformations. *But what happened to the matrix* $\mathbf{Q}$? One can show that

$$
\mathbf{Q}^\top = \mathbf{H}_{p} \mathbf{H}_{p-1} \cdots \mathbf{H}_2 \mathbf{H}_1,
$$

where the order of multiplication reflects the order in which we apply reflectors to eliminate columns of $\mathbf{X}$. Thus, we can recover $\mathbf{Q}$ by initializing $\mathbf{Q}_{0} = \mathbf{I}_{n}$ and applying the same Householder transformations used in Gaussian elimination to recover $\mathbf{Q}^\top$.

We will now put everything together to implement a QR algorithm using Householder transformations.

Write a function called `compute_household_u()` which implements the logic of calculating the vector defining $\mathbf{H}_{j}$ for column $j$.

```{r}
compute_household_u <- function(X, j, tol = sqrt(.Machine$double.eps)) {
  # define z as a subvector of X[,j], from diagonal to end
  z <- X[,j][j:length(X[,j])]
  
  # calculate u tilde (unnormalized)
  znorm <- norm(as.matrix(z), type = "e")
  yHat <- c(1, rep(0,length(z)-1))
  uTilde <- 0.5 * (z/znorm - yHat)
  # check if u is 0, if it is return 0 vector
  if (sum(abs(uTilde) < tol) > 0 ) {
    return (rep(0, length(X[,j])))
  }
  
  # normalize u tilde
  uTilde <- uTilde / norm(as.matrix(uTilde), type ="e")
  # construct u
  u <- c(rep(0, j-1), uTilde)
  
  return (u)
}
```

Use this helper function to implement `qr_householder()`, which computes `Q` and `R` such that `Q %*% R` $\approx$ `X` for a given matrix `X`.

```{r}
qr_householder <- function(X, ...) {
  n <- nrow(X)
  p <- ncol(X)
  R <- X
  Qt <- diag(rep(1, n))

  # compute R
  for (j in 1:p) {
    # compute new u for each column of R
    u <- compute_household_u(R, j)
    # apply householder to each column of Q with new u
    Qt <- apply(Qt,2,householder, u = u)
    # compute new R
    R <- eliminate_col2(R, j)
  }

  return(list(Q = t(Qt), R = R))
}
```

**Test problem 1:** Try it on our `X` matrix. Sample output is provided below. *Note that the* $\mathbf{Q}$ and $\mathbf{R}$ factors are only unique up to sign. It should be similar to `qr.Q(qr(X))` and `qr.R(qr(X))`.

```{r}
QR <- qr_householder(X)
QR
# $Q
#           [,1]       [,2]       [,3]
# [1,] 0.2672612  0.8728716 -0.4082483
# [2,] 0.5345225  0.2182179  0.8164966
# [3,] 0.8017837 -0.4364358 -0.4082483
#
# $R
#          [,1]         [,2]         [,3]
# [1,] 3.741657 8.552360e+00 1.336306e+01
# [2,] 0.000000 1.963961e+00 3.927922e+00
# [3,] 0.000000 2.220446e-16 8.881784e-16
```

**Test problem 2:** A good implementation should achieve errors smaller than $10^{-13}$ on the following test problem.

```{r}
set.seed(100)
n <- 100
p <- 10
A <- matrix(rnorm(n*p) ^ 2, nrow = n, ncol = p)

decomp <- qr_householder(A)

# orthogonality
err1 <- max(abs(t(decomp$Q) %*% decomp$Q - diag(rep(1, n))))
cat("Maximum absolute error in checking orthogonality: ", err1, "\n")

# approximation
err2 <- max(abs(decomp$Q %*% decomp$R - A))
cat("Maximum absolute error in checking approximation: ", err2, "\n")
```

## Part (e)

(e) Benchmark your code using the `microbenchmark` library. Use the test problem above, `A`, and compare against `qr()` with `LAPACK = TRUE`. Dr. Landeros' implementation has a median run time of about 20000 $\mu$s (minimum 16000 $\mu$s) in R, compared to about 10 $\mu$s (minimum 10 $\mu$s) using R's `qr()`.

```{r}
library(microbenchmark)

microbenchmark(matrix(rnorm(n*p) ^ 2, nrow = n, ncol = p),
               qr_householder(X),
               qr(X, LAPACK = TRUE))
```

# Problem 4: Assessing computational complexity

## Part (a)

(a) Say that $\mathbf{u}$ occurring in $\mathbf{H}$ has dimension $n$. State the *exact* complexity of computing $\mathbf{H} \mathbf{x}$ for arbitrary $\mathbf{x} \in \mathbb{R}^{n}$.

::: callout-tip
## Hint

Remember that we should not think of $\mathbf{H}$ as an explicit matrix, so don't count operations that involve multiplying by 0. Exploit the structure of $\mathbf{H}$ is counting flops.
:::

**Solution**

-   $c = \mathbf{u}^\top \mathbf{x}$: 2n-1 flops
-   $c \cdot \mathbf{u}$: n flops
-   $\mathbf{x} - c \mathbf{u}$: n flops
-   Total: 4n-1 flops

## Part (b)

(b) State the leading order complexity of forming the unit sub-vector $\tilde{\mathbf{u}}_{j}$, at step $j$, used to eliminate subdiagonals on column $j$. *Do not count copying or 0-padding operations*.

**Solution**

Calculating znorm: 2(n - j + 1)

Calculating uTilde: 3(n - j + 1)

Total: 5(n - j + 1)

## Part (c)

(c) Using the previous two parts, state the leading order complexity of QR decomposition with Householder transformations. *Ignore the cost of computing* $\mathbf{Q}^\top$ altogether, and do not count copying or 0-padding operations.

::: callout-tip
## Hints

1.  Specifically, do not count the flops associated with multiplying by the 0-padding in $\mathbf{u}_j$.
2.  Do not make assumptions about the relative sizes of $n$ and $p$.
3.  Recall that $\sum_{i=1}^{k} i^2 = \frac{1}{6}k(k+1)(2k+1)$.
:::

**Solution**

-   5(n - j - 1) flops for computing $\mathbf{u}_j$ according to part (b).

$$
\sum_{j=1}^p p[5(n-j+1) + p(4n-1) + 5(n-j+1) + p(4n-1)]
$$

$$
= \sum_{j=1}^p p(10n - 10j + 10 + 8np - 2p)
$$

$$
= p^2(10n+10+8np-2p) - 10\sum_{j=1}^p j
$$

$$
= 8np^3-2p^3+10np^2+10p^2 - 5p(p+1)
$$

-   $8np^3-2p^3+10np^2+5p^2 - 5p$ flops for computing $\mathbf{H} \mathbf{X}$ with $\mathbf{X} \in \mathbb{R}^{k \times (p-j+1)}$.
-   Leading order: $8np^3$ flops.
